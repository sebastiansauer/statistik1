# Geradenmodelle 1 {#sec-gerade1}


```{r libs-hideen}
#| include: false
library(gt)
library(ggrepel)
library(tidyverse)
library(easystats)
```


```{r color-theme}
#| include: false
theme_set(theme_minimal())
scale_colour_discrete <- function(...) 
  scale_color_okabeito()
```



```{r}
#| include: false

source("_common.R")
```





{{< include children/colors.qmd >}}




## Einstieg



In diesem Kapitel ben√∂tigen Sie die √ºblichen R-Pakete (`tidyverse`, `easystats`) und Daten (`mariokart`),  s. @sec-import-mariokart.






### Lernziele


- Sie k√∂nnen ein Punktmodell von einem Geradenmodell begrifflich unterscheiden.
- Sie k√∂nnen die Bestandteile eines Geradenmodells aufz√§hlen und erl√§utern.
- Sie k√∂nnen die G√ºte eines Geradenmodells anhand von Kennzahlen bestimmen.
- Sie k√∂nnen Geradenmodelle sowie ihre Modellg√ºte in R berechnen.





::: {.content-visible when-format="html"}

```{r import-mariokart-csv}
mariokart <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv")
```
:::



## Vorhersagen


Vorhersagen sind eine n√ºtzliche Sache, 
unter (mindestens) folgenden Voraussetzungen:

1. Sie sind pr√§zise.
2. Wir wissen, wie pr√§zise.
3. Jemand interessiert sich f√ºr die Vorhersage.


Die Methode des Vorhersagens, die wir hier betrachten, 
nennt man auch *lineare Regression*.


### Vorhersagen ohne Pr√§diktor

:::::{#exm-noten-prognose}
Nach intensiver Besch√§ftigung mit Statistik sind Sie allgemein als Checker bekannt.
Viele j√ºngere Studis fragen Sie um Rat.
Eines Tages kommt eine Studentin zu Ihnen, Toni, und fragt: "Welche Statistiknote kann ich in der Klausur erwarten?"
Sie entgegnen: "Wie viel hast du denn gelernt?".
Die Antwort: "Sage ich nicht."
Nach kurzem √úberlegen geben Sie den Notenschnitt der letzten Klausur als Prognose f√ºr diese Person. 
Dazu rechnen Sie schnell den Notenschnitt (Mittelwert) aus.



:::: {.content-visible when-format="html" unless-format="epub"}


Zuerst importieren Sie die Daten der letzten Klausur. Die Syntax in @lst-noten2-lokal wird bei Ihnen nur funktionieren, 
wenn auf *Ihrem Computer* dieser Ordner mit dieser Datei existiert. 
Andernfalls m√ºssen Sie die Daten erst herunterladen^[<https://raw.githubusercontent.com/sebastiansauer/statistik1/main/data/noten.csv>]:

```{r}
#| echo: false
# set.seed(42)
# noten2 <-
#   tibble(
#     id = 1:100,
#     x = rnorm(100, mean = 73, sd = 10),
#     y = x + rnorm(100, 0, 10)
#   )
# write.csv(noten2, "data/noten2.csv")
```


```{r}
#| lst-label: lst-noten2-lokal
#| lst-cap: "Der Datensatz 'noten2' liegt im Unterordner 'Noten.'"
noten2 <- read.csv("data/noten2.csv")
```

{{< downloadthis data/noten2.csv dname = "noten2" >}}


```{r}
noten2 %>% 
  summarise(mw = mean(y))  # y ist der Punktwert in der Klausur
```
::::




Ihre Antwort lautet: 
"Im Schnitt haben die Studis bei der letzten Klausur ungef√§hr `r mean(round(noten2$y))` erzielt. 
Diesen Wert kannst du erwarten. 
Solange ich keine genaueren Infos habe, 
z.B. wie viel du gelernt hast, kann ich dir keine genauere Vorhersage machen. Sorry!" $\square$
:::::


Ohne Kenntnis eines Pr√§diktors (UV) (wie z.B. Lernzeit) 
ist der Mittelwert ein geeigneter Vorhersagewert f√ºr jede Beobachtung, s. @fig-noten3.
Wir nutzen den Mittelwert als Punktmodell f√ºr den Klausurerfolg. $\square$


```{r p-punktmodell}
#| echo: false
#| fig-cap: "Mittelwert als Vorhersagewert, bzw. Mittelwert als Punktmodell"
#| label: fig-noten3
#| out-width: 50%


ggplot(noten2) +
  aes(id, y) +
  geom_point() +
  geom_hline(yintercept = mean(noten2$y), color = modelcol) +
  annotate("label", x = -Inf, y = mean(noten2$y), 
           label = paste0("MW: ", round(mean(noten2$y))), 
           hjust = 0,
           size = 6) +
  theme_minimal()
```

:::{#def-nullmodell}
### Nullmodell (Punktmodell)
Modelle ohne Pr√§diktor, Punktmodelle also, 
kann man so bezeichnen: `y ~ 1`. 
Da das Modell null Pr√§diktoren hat, 
nennt man es auch manchmal "Nullmodell".
:::

Auf Errisch kann man dieses Nullmodell so spezifizieren:

```{r lm0}
#| results: show
lm0 <- lm(y ~ 1, data = noten2)
lm0
```

`lm` steht f√ºr "lineares Modell", die `1` sagt, 
dass es keine Pr√§diktoren gibt.
In dem Fall wird der Mittelwert, `r round(mean(noten2$y))`, als Gerade verwendet.
Der zur√ºckgemeldete Koeffizient `(Intercept)` 
ist hier der einizige des Punktmodells.
Da es ein Punktmodell ist, sagt es f√ºr alle Beobachtungen (hier Studentis) den gleichen Wert vorher, n√§mlich `r round(mean(noten2$y))`.


### Vorhersagen mit Pr√§diktor




:::{#exm-noten3}
### Toni verr√§t die Lernzeit

Toni entschlie√üt sich dann doch noch, die Lernzeit zu verraten:
"Okay, also ich hab insgesamt 42 Stunden gelernt, insgesamt."
Jetzt m√ºssen Sie erstmal nachdenken: 
"Wie viele Klausurpunkte sage ich vorher, wenn Toni 42 Stunden gelernt hat?"

Sie visualisieren sich zur Hilfe die vorliegenden Daten, s. @fig-noten4, (a).


```{r}
#| eval: false
library(DataExplorer)
noten2 %>% 
  plot_scatterplot(by = "y")  # Y-Variable muss angegeben werden
```


```{r lm1-b0-b1}
#| echo: false
lm1 <- lm(y ~ x, data = noten2)
lm1_b0 <- coef(lm1)[1]
lm1_b2 <- coef(lm1)[2]

toni_punkte <- predict(lm1, newdata = data.frame(x=42))
```


Auf dieser Basis antworten Sie Toni: 
"Bei 42 Stunden Lernzeit solltest du so `r round(toni_punkte, 0)` Punkte bekommen. 
K√∂nnte mit dem Bestehen eng werden."
Toni ist nicht begeistert von Ihrer Prognose und zieht von dannen. $\square$
:::


Der "Trend" (im Sinne eines linearen Zusammenhangs) von *Lernzeit* und *Klausurpunkte* ist deutlich zu erkennen:
Je mehr Lernzeit, desto mehr Klausurpunkte.
Mit einem Lineal k√∂nnte man eine entsprechende Gerade in das Streudiagramm einzeichnen, s. @fig-noten4, (b).


```{r fig-noten}
#| label: fig-noten4
#| echo: false
#| fig-cap: "Noten und Lernzeit: Rohdaten (a) und mit Modell (b). Mittelwerte sind mit gestrichelten Linien eingezeichnet. Tonis Vorhersage ist mit einem Punkt markiert."
#| layout-ncol: 2
#| fig-subcap: 
#|   - "Streudiagramm"
#|   - "Streudigramm mit 'Trendgerade' (blau)"

#noten2 <- read.csv(noten2, "data/noten2.csv")



ggplot(noten2) +
  aes(x, y) +
  geom_point() +
  labs(x = "Lernzeit",
       y = "Klausurpunkte") +
  theme_minimal() +
  theme_large_text()

noten2 %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_vline(xintercept = mean(noten2$x), linetype = "dashed", color = "grey") +  
  geom_hline(yintercept = mean(noten2$y), linetype = "dashed", color = "grey") +   
  geom_abline(slope = coef(lm1)[2], intercept = coef(lm1)[1], color = modelcol, size = 1.5) +
  theme_minimal() +
  annotate("label", x = mean(noten2$x), y = -Inf, 
           label = paste0("MW: ", round(mean(noten2$x))), vjust = "bottom") +
  annotate("label", y = mean(noten2$y), x = -Inf, 
           label = paste0("MW: ", round(mean(noten2$y))), hjust = "left")   +
  annotate("point", x = 42, y = toni_punkte, color = ycol,
           alpha = .7, size = 7) +
  scale_x_continuous(breaks = c(20, 40, 60, 80, 100)) +
  labs(x = "Lernzeit",
       y = "Klausurpunkte") +
  theme_large_text()
```




## Geradenmodelle


### Achsenabschnitt und Steigung definieren eine Gerade

Wir verwenden eine Gerade als Modell f√ºr die Daten, s. @fig-noten4, b.
Anders gesagt: 
Wir modellieren die Daten (bzw. deren Zusammenhang) mit einer Geraden.
Ein *Geradenmodell* ist eine Verallgemeinerung des Punktmodells:
Ein Punktmodell sagt f√ºr alle Beobachtungen den gleichen Wert vorher.
@fig-noten3 und @fig-noten4 stellen ein Punktmodell 
einem Geradenmodell gegen√ºber.

In einem Geradenmodell wird nicht mehr (notwendig) 
f√ºr jede Beobachtung die gleiche Vorhersage $\hat{y}$ gemacht (wie das bei einem Punktmodell der Fall ist).

::::{#def-gerade}
### Gerade

Eine Gerade ist das, was man bekommt, 
wenn man eine lineare Funktion in ein Koordinatensystem einzeichnet.
Man kann sie durch durch zwei *Koeffizienten* festlegen: 
Achsenabschnitt (engl. *intercept*), und Steigung (engl. *slope*).
Manchmal wird (z.B. im Schulunterricht) der Achsenabschnitt mit $t$ und die Steigung mit $m$ bezeichnet:
 
::: {.content-visible unless-format="epub"}

<!-- HTML, PDF: -->

$f(\color{xcol}{x})=\color{ycol}{y}={m} \color{xcol}{x} + \color{beta0col}{t}$.

In der Statistik wird folgende Nomenklatur bevorzugt:  $f(\color{xcol}{x})=\color{ycol}{\hat{y}}=\color{beta0col}{\beta_0} + {\beta_1} \color{xcol}{x}$ oder $f(\color{xcol}{x})=\color{ycol}{\hat{y}}= \color{beta0col}{b_0} + {b_1} \color{xcol}{x}$ .

Die Nomenklatur mit $\color{beta0col}{b_0}, \color{beta1col}{b_1}$ hat den Vorteil, dass man das Modell einfach erweitern kann: $b_2, b_3, ...$. Anstelle von $b$ liest man auch oft $\beta$. 
Griechische Buchstaben werden meist verwendet, um zu zeigen, 
dass man an einer Aussage √ºber eine Population, 
nicht nur √ºber eine Stichprobe, machen m√∂chte.

Das "Dach" √ºber y, $\color{modelcol}{\hat{y}}$, 
dr√ºckt aus, dass es sich den den gesch√§tzten, bzw. vom Modell vorhergesagten ("modellierten") 
Wert f√ºr $\color{ycol}{y}$ handelt, nicht der tats√§chliche (empirische, beobachtete) Wert von $\color{ycol}{y}$. 
@fig-regrtex skizziert die Elemente einer Regression. 
:::



 
::: {.content-visible when-format="epub"}

<!-- NUR EPUB: -->
<!-- KEINE FARBEN -->


$f(x)={y}=mx + t.$

In der Statistik wird folgende Nomenklatur bevorzugt:  $f(x)={\hat{y}}={\beta_0} + {\beta_1}$ oder $f(x)=y= {b_0} + {b_1}.$ 

Die Nomenklatur mit $b_0, b_1$ hat den Vorteil, dass man das Modell einfach erweitern kann: $b_2, b_3, ...$. Anstelle von $b$ liest man auch oft $\beta$. 
Griechische Buchstaben werden meist verwendet, um zu zeigen, 
dass man an einer Aussage √ºber eine Population, 
nicht nur √ºber eine Stichprobe, machen m√∂chte.

Das "Dach" √ºber y, $\hat{y}$, 
dr√ºckt aus, dass es sich den den gesch√§tzten, bzw. vom Modell vorhergesagten ("modellierten") 
Wert f√ºr $y$ handelt, nicht das tats√§chliche (empirische, beobachtete) $y$. $\square$
@fig-regrtex skizziert die Elemente einer Regression. 
:::

::::







![Achsenabschnitt ($\beta_0$) und Steigung ($\beta_1$) einer Regressionsgeraden [@menk_linear_2014]](img/regr.png){#fig-regrtex width="70%"}


::::: {.content-visible unless-format="epub"}

<!-- HTML, PDF -->


:::{#def-einfache-lineare-modell}

### Das einfache lineare Modell

Das einfache lineare Modell beschreibt den Wert einer abh√§ngigen metrischen Variablen, $\color{ycol}{y}$,
als lineare Funktion von einer (oder mehreren) unabh√§ngigen Variablen, $\color{xcol}{x}$, plus einem Fehlerterm, $\color{errorcol}{e}$ bzw. $\color{errorcol}{\epsilon}$, s. @eq-linear-model. $\square$
:::




$$\begin{aligned}
\color{ycol}{y} &= f(\color{xcol}{x}) + \color{errorcol}{\epsilon} \\
\color{ycol}{y_i} &= \color{beta0col}{\beta_0} + \color{beta1col}{\beta_1} \cdot \color{modelcol}{x_i} + \color{errorcol}{\epsilon_i} \square
\end{aligned}$${#eq-linear-model}

Mit:

- $\color{beta0col}{\beta_0}$: gesch√§tzter y-Achsenabschnitt laut Modell (engl. *intercept*)
- $\color{beta1col}{\beta_1}$: gesch√§tzte Steigung (Regressionsgewicht) laut Modell (engl. *slope*)
- $\color{errorcol}{\epsilon}$: Fehler des Modells

In @eq-linear-model schreiben wir $\color{ycol}{y}$ und nicht $\color{modelcol}{\hat{y}}$, 
weil wir den tats√§chlichen, beobachteten Wert von  $\color{ycol}{y}$  als Summe von vorhergesagtem Wert, $\color{modelcol}{\hat{y}}$ und Modellfehler, $\color{errorcol}{\epsilon}$ beschreiben.


:::::





::::: {.content-visible when-format="epub"}

<!-- epub -->
<!-- KEINE FARBEN -->




$$\begin{aligned}
{y} &= f({x}) + {\epsilon} \\
{y_i} &= {\beta_0} + {\beta_1} \cdot {x_i} + {\epsilon_i} \square
\end{aligned}$$

Mit:

- ${\beta_0}$: gesch√§tzter y-Achsenabschnitt laut Modell
- ${\beta_1}$: gesch√§tzte Steigung laut Modell
- ${\epsilon}$: Fehler des Modells


:::::






Je nach Datenlage k√∂nnen sich Regressionsgeraden in Steigung oder Achsenabschnitt unterscheiden, s. @fig-regr-div.

```{r lm11-lm12}
#| echo: false
#| cache: false

set.seed(42)
d1 <- tibble::tibble(
  x1 = rnorm(50),
  y1 = 2 * x1 + rnorm(50, mean = 0, sd = .5)
)

set.seed(3141)
d2 <- tibble::tibble(
  x2 = rnorm(50),
  y2 = -x2 + rnorm(50, mean = 1, sd = .5)
)

lm11 <- lm(y1 ~ x1, data = d1)
lm11_coef <- coef(lm11)
lm12 <- lm(y2 ~ x2, data = d2)
lm12_coef <- coef(lm12)

cap1 <- paste0("Datensatz 1: b0 = ", round(lm11_coef[1], 2), "; b1 = ", round(lm11_coef[2], 2))
cap2 <- paste0("Datensatz 2: b0 = ", round(lm12_coef[1], 2), "; b1 = ", round(lm12_coef[2], 2))
```


```{r p-b0-b1}
#| echo: false
#| cache: false
#| fig-cap: "Regressionsanalysen mit verschiedenen Koeffizienten, aber gleicher Modellg√ºte"
#| label: fig-regr-div
#| layout-ncol: 2
#| fig-subcap: 
#|   - Datensatz 1
#|   - Datensatz 2

ggplot(d1, aes(x1, y1)) + 
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, color = okabeito_colors()[8]) +
  scale_y_continuous(limits = c(-4, 4)) +
  labs(title = paste0("b0 = ",round(lm11_coef[1], 2), "; b1 = ", round(lm11_coef[2]), 2)) +
  theme_modern() +
theme(plot.title = element_text(size = 16))

ggplot(d2, aes(x2, y2)) + 
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, color = okabeito_colors()[8]) +
  scale_y_continuous(limits = c(-4, 4)) +
  labs(title = paste0("b0 = ",round(lm12_coef[1], 1), "; b1 = ", round(lm12_coef[2]), 2)) +
  theme_modern() +
theme(plot.title = element_text(size = 16))
```


::::: {.content-visible when-format="html" unless-format="epub"}

@fig-linfun [@yi_interactive_2021] zeigt ein interaktives Beispiel einer linearen Funktion. 
Sie k√∂nnen Punkte per Klick/Touch hinzuf√ºgen.


::::{#fig-linfun}

::: {.figure-content}



{{< include children/regression-interactive.qmd >}}



:::

Interaktives Beispiel f√ºr eines lineares Modell. F√ºgen Sie Punkte per Klick/Touch hinzu.
::::
:::::



:::{#exm-noten5}
### Toni will es genau wissen
Da Toni Sie als Statistik-Profi abgespeichert hat, 
werden Sie wieder konsultiert:
"Okay, ich hab noch zwei Fragen. 
Erstens: Wie viele Punkte bekomme ich, wenn ich gar nicht lerne? 
Zweitens, wie viele Punkte bekomme ich pro gelernte Stunde? Ist immerhin meine Lebenszeit, krieg ich nicht zur√ºck!"
[Das sind gute Fragen. Den $\color{ycol}{Y}$-Wert (Klausurpunkte) bei $\color{xcol}{x}=0$ gibt der Achsenabschnitt zur√ºck. ]{.content-visible unless-format="epub"}
[Das sind gute Fragen. Den ${Y}$-Wert (Klausurpunkte) bei ${x}=0$ gibt der Achsenabschnitt zur√ºck. ]{.content-visible when-format="epub"}
Schnell skizzieren Sie dazu ein Diagramm, s. @fig-beta0.
Puh, die Antwort wird Toni nicht gefallen ... $\square$
:::

```{r fig-beta0}
#| echo: false
#| label: fig-beta0
#| out-width: 75%
#| fig-cap: "Der Achsenabschnitt: Wie viele Punkt kann Toni erwarten bei 0 Lernstunden? (roter Punkt bei x=0)"

lm1 <- lm(y ~ x, data = noten2)
lm1_b0 <- coef(lm1)[1]

noten2 %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_vline(xintercept = mean(noten2$x), linetype = "dashed", color = "grey") +  
  geom_hline(yintercept = mean(noten2$y), linetype = "dashed", color = "grey") +   
  geom_abline(slope = coef(lm1)[2], intercept = coef(lm1)[1], color = okabeito_colors()[8], size = 1.5) +
  theme_minimal() +
  #scale_x_continuous(limits = c(0, 100)) +
  #scale_y_continuous(limits = c(0, 100)) +
  annotate("point", x = 0, y = lm1_b0, color = modelcol, size = 7, alpha = .7) +
  annotate("label", x = mean(noten2$x), y = 0, vjust = "bottom",
           label = "MW Lernzeit") +
  annotate("label", y = mean(noten2$y), x = 12, vjust = "left",
           label = "MW Klausurpunkte") +
  geom_text_repel(data = tibble(x = 0, y = predict(lm1, newdata = tibble(x = 0))), 
                  aes(x = x, y = y,), 
                  label = "Toni",
                  point.padding = .5,
                  size = 8) +
  labs(x = "Lernzeit",
       y = "Klausurpunkte")
```



Anstelle auf @fig-beta0 zu schauen, 
k√∂nnen Sie sich auch von R Tonis Klausurerfolg vorhersagen (to predict) lassen:


>    [üßë‚Äçüè´]{.content-visible when-format="html"}[\emoji{teacher}]{.content-visible when-format="pdf"}  Hey R, predicte mir mal auf Basis vom Modell "lm1" den Lernerfolg f√ºr Toni, wenn der x=0 Stunden lernt.

>    [ü§ñ]{.content-visible when-format="html"}[\emoji{robot}]{.content-visible when-format="pdf"} Okay, ich predicte mit Modell "lm1" und nehme als neue Datentabelle Tonis Lernzeit (x=0)!

```{r}
tonis_lernzeit <- tibble(x = 0)  # `tibble` erstellt einen Dataframe
```


```{r}
predict(lm1, newdata = tonis_lernzeit)
```


`predict` erwartet f√ºr das Argument `newdata` einen Dataframe.
In diesem Beispiel hei√üt er `tonis_lernzeit`.

### Spezifikation eines Geradenmodells


Ein Geradenmodell kann man im einfachsten Fall spezifizieren wie s. @eq-mod1 dargestellt.


::: {.content-visible unless-format="epub"}

$$\color{ycol}{\hat{y}} \sim \color{xcol}{\text{x}}$$ {#eq-mod1}

Lies: "Laut meinem Modell ist mein (gesch√§tztes) $\color{ycol}{\hat{y}}$ irgendeine Funktion von $\color{xcol}{\text{x}}$".

Wir erinnern uns, dass $\color{ycol}{Y}$ die $\color{ycol}{AV}$ und $\color{xcol}{X}$ die $\color{xcol}{UV}$ ist:

$$\color{ycol}{AV} \sim \color{xcol}{UV}$$ {#eq-mod1}

:::


::: {.content-visible when-format="epub"}
$${\hat{y}} \sim {\text{x}}$$ {#eq-mod1}

Lies: "Laut meinem Modell ist mein (gesch√§tztes) ${\hat{y}}$ irgendeine Funktion von ${\text{x}}$".

Wir erinnern uns, dass ${Y}$ die ${AV}$ und ${X}$ die ${UV}$ ist:

$${AV} \sim {UV}$$ {#eq-mod1}
:::


Wir werden als Funktion nur Geraden verwenden.
Die genauen Werte der Gerade lassen wir uns  vom Computer ausrechnen.
@eq-mod1 k√∂nnen Sie so ins Errische √ºbersetzen:


```{r}
#| eval: false
lm(y ~ x, data = meine_daten)
```

`lm` steht f√ºr "lineares Modell", also eine Gerade als Modell.
Die Gerade nennt man auch *Regressionsgerade* (an anderer Stelle in diesem Buch unscharf als "Trendgerade" bezeichnet).

:::{#exm-noten5}
### Zahlen f√ºr Toni
Toni ist nicht zufrieden mit Ihren Vorhersagen: "Jetzt h√∂r mal auf mit deinem Lineal hier herum zu malen. Ich will es genau wissen, sage mir pr√§zise Zahlen!".
:::


```{r}
lm1 <- lm(y ~ x, data = noten2)
lm1
```

R gibt Ihnen die beiden Koeffizienten f√ºr die Gerade aus. 
Den Namen des Objekts k√∂nnen Sie frei aussuchen, z.B. `mein_erstes_lm`.
Die Regressionsgleichung lautet demnach:
`y_pred = 8.6 + 0.88*x`.


::: {.content-visible unless-format="epub"}
`8.6` ist der Achsenabschnitt, d.h. der Wert von $\color{ycol}{Y}$ wenn $\color{xcol}{x}=0$.
`0.88` ist das Regressionsgewicht, d.h. die Steigung der Regressionsgeraden: F√ºr jede Stunde Lernzeit steigt der vorhergesagte Klausurerfolg um `0.88` Punkte.

Mit Kenntnis der beiden Koeffizienten kann man beliebige $\color{ycol}{Y}$-Werte ausrechnen, gegeben bestimmte $\color{xcol}{X}$-Werte.
Hat jemand zum Beispiel 10 Stunden gelernt, 
w√ºrden wir folgendes Klausurergebnis vorhersagen:

```{r}
lernzeit <- 10
y_pred <- 46 + 0.88*lernzeit
y_pred
```
:::


::: {.content-visible when-format="epub"}

`8.6` ist der Achsenabschnitt, d.h. der Wert von ${Y}$ wenn ${x}=0$.
`0.88` ist das Regressionsgewicht, d.h. die Steigung der Regressionsgeraden: F√ºr jede Stunde Lernzeit steigt der vorhergesagte Klausurerfolg um `0.88` Punkte.

Mit Kenntnis der beiden Koeffizienten kann man beliebige ${Y}$-Werte ausrechnen gegeben bestimmte ${X}$-Werte.
Hat jemand zum Beispiel 10 Stunden gelernt, 
w√ºrden wir folgendes Klausurergebnis vorhersagen:

```{r}
lernzeit <- 10
y_pred <- 8.6 + 0.88*lernzeit
y_pred
```
:::


:::{#exm-noten6}
### Vorhersage f√ºr Klausurerfolg, n√§chster Versuch
Sie versuchen, noch etwas Gutes f√ºr Toni zu tun.
R hilft Ihnen dabei und rechnet die erwartete Punktzahl aus, wenn Toni 73 Stunden lernt.
Sie d√ºrfen es aber auch selber rechnen, wenn Ihnen das lieber ist.
:::


```{r}
tonis_lernzeit2 <- tibble(x = 73)  
```

`tonis_lernzeit2` ist eine Tabelle mit einer Zeile und einer Spalte:
```{r}
tonis_lernzeit2
```


```{r}
predict(lm1, newdata = tonis_lernzeit2)
```



Die Syntax von `predict` lautet:

```
predict(modell, newdata = tabelle_mit_pr√§diktorwerten)
```

Die Funktion `predict` liefert eine Vorhersage f√ºr ein ein Modell,
z.B. `lm1` und f√ºr einen bestimmten Dataframe (der die Pr√§diktorwerte enthalten muss).



### Vorhersagefehler


::: {.content-visible unless-format="epub"}
Die Differenz zwischen vorhergesagtem Wert f√ºr eine (neue) Beobachtung, $\color{modelcol}{\hat{y_0}}$ 
und ihrem tats√§chlichen Wert nennt man Vorhersagefehler (error, $e$) oder *Residuum*: 
$\color{errorcol}{e_i} = \color{ycol}{y_i} - \color{modelcol}{\hat{y}_i}$.
:::



::: {.content-visible when-format="epub"}
Die Differenz zwischen vorhergesagten Wert f√ºr eine (neue) Beobachtung, ${\hat{y_0}}$ 
und ihrem tats√§chlichen Wert nennt man Vorhersagefehler (error, $e_i$) oder *Residuum*: 
${e_i} = {y_i} - {\hat{y}_i}$.
:::



```{r p-abweichungsbalken}
#| echo: false
#| fig-cap: "Vorhersagefehler als Abweichungsbalken. (a) Beim Geradenmodell, sind die Vorhersagefehler (Abweichungsbalken) kleiner (k√ºrzer) als in (b), beim Punktmodell."
#| label: fig-resid
#| layout-ncol: 2
#| fig-subcap: 
#|   - Geradenmodell (lm1)
#|   - Punktmodell (lm0)

noten2 <-
  noten2 %>% 
  mutate(yhat = predict(lm1, newdata = noten2))

noten2 %>% 
  ggplot(aes(x, y)) +
  geom_point(color = ycol) +
  geom_abline(slope = coef(lm1)[2], intercept = coef(lm1)[1], color = modelcol, size = 1.5) +
  theme_minimal() +
  geom_segment(aes(xend = x, yend = yhat), color = errorcol) +
theme_large_text()

noten2 %>% 
  ggplot(aes(x, y)) +
  geom_point(color = ycol) +
  geom_hline(yintercept = mean(noten2$y), color = modelcol, size = 1.5) +
  theme_minimal() +
  geom_segment(aes(xend = x, yend = mean(noten2$y)), color = errorcol)  +
theme_large_text()

```


Wie ist es mit den Vorhersagefehlern von beiden Modellen bestellt?
Lassen wir uns von R die Streuung (Residuen) 
in Form der mittleren Absolutabweichung (MAE) ausgeben (aus dem Paket `easystats`):

```{r}
mae(lm0)
mae(lm1)
```


Vergleichen wir MAE im  Nullmodell mit MAE in `lm1`: 

```{r}
verhaeltnis_fehler_mae <- mae(lm1) / mae(lm0)
verhaeltnis_fehler_mae
```



Ah! Das Geradenmodell ist viel besser:
Von `lm0` zu `lm1` haben die mittlere Absolutl√§nge des Fehlerbalkens auf `r round(verhaeltnis_fehler_mae, 2)*100` Prozent verbessert.
Nicht schlecht!


:::{#def-fehlerstreung}
### Fehlerstreuung
Als Fehlerstreuung bezeichnen wir die Gesamtheit der Abweichungen der beobachteten Werte ($y_i$) vom vorhergesagten Wert ($\hat{y}_i$). $\square$
:::

Zur Berechnung der Fehlerstreuung gibt es mehrere Kenngr√∂√üen 
wie MAE oder MSE.
Ein Geradenmodell ist immer besser als ein Punktmodell 
(im Hinblick auf die Verringerung der Fehlerstreuung), 
solange X mit Y korreliert ist. 


Nat√ºrlich k√∂nnen wir -- in Analogie zur Varianz -- 
auch den mittleren Quadratfehlerbalken (Mean Squared Error, MSE) berechnen.
Wer mag, 
kann den MSE auch von Hand berechnen: `mean((noten2$y - mean(noten2$y))^2)`.

```{r}
mse(lm0)
mse(lm1)
```


```{r}
verhaeltnis_fehler_mse <- mse(lm1)/mse(lm0)
verhaeltnis_fehler_mse
```

Betrachtet man die MSE, so kann man eine Verbesserung um `r 1-verhaeltnis_fehler_mse` auf `r verhaeltnis_fehler_mse` feststellen.


### Berechnung der Modellkoeffizienten

Aber wie legt man die Regressionsgerade in das Streudiagramm, bildlich gesprochen?

Die Regressionskoeffizienten (hier synonym: Modellparameter) $\beta_0$ und $\beta_1$ w√§hlt man so, dass die *Residuen* *minimal* sind.
Genauer gesagt wird die Summe der quadrierten [Residuen]{.green} minimiert, s. @eq-min.


:::: {.content-visible when-format="html" unless-format="epub"}


 @fig-opt veranschaulicht die Minimierung der Residuen (Vorhersagefehler).

::::{#fig-opt}

:::{.panel-tabset}

### Minimierung der Residuen

![Berechnung der Modellkoeffizienten durch Minimierung der Residuen](img/RegressionSpring.gif)

### Minimierung der quadrierten Residuen

![Minimierung der quadrierten Residuen](img/RegressionRSS.gif)

:::


Bildquelle: [Karsten L√ºbke, FOM Hochschule](https://www.fom-blog.de/autorinnen-und-autoren/karsten-luebke)

::::
:::::




::: {.content-visible unless-format="epub"}
$$\text{min}\sum_i \color{errorcol}{e_i}^2$${#eq-min}
:::


::: {.content-visible when-format="epub"}
$$\text{min}\sum_i {e_i}^2$${#eq-min}
:::

Es gibt verschiedene Methoden, 
um die Koeffizienten zu berechnen (die aber nicht in diesem Buch zu finden sind).
Eine sch√∂ne Darstellung dazu findet sich bei @kaplan_statistical_2009.


::: {.content-visible when-format="html" unless-format="epub"}
"Von Hand" k√∂nnen Sie die Optimierung von $\beta_0$ und $\beta1$ in 
 dieser App der FOM-Hochschule^[<https://fomshinyapps.shinyapps.io/KleinsteQuadrate/>] ausprobieren.
:::








## R-Quadrat als Ma√ü der Modellg√ºte

Das Modell `lm1` weist noch `r verhaeltnis_fehler_mse` der Fehlerstreuung (MSE) des Nullmodells auf.
Anders gesagt, wir haben uns ( (bzw. das Modell hat sich) um $1 - `r round(verhaeltnis_fehler_mse, 2)`$ verbessert.

```{r}
1 - verhaeltnis_fehler_mse
```




:::{#def-r2}
### R-Quadrat
Der Anteil der Verringerung (als Anteil) der Fehlerstreuung der Zielvariablen  zwischen `lm0` und dem
 gerade untersuchten Modell nennt man  *R-Quadrat* ($R^2$).
Das R-Quadrat ($R^2$) eines Modells $m$ ist definiert als die Verringerung der Streuung, 
wenn man das Modell $m$ mit dem Nullmodell $m_0$ vergleicht: $R^2 =1-  \frac{\text{MSE}_{m}}{\text{MSE}_{m0}}$. 
R-Quadrat ist ein Ma√ü der *Modellg√ºte*: 
Je gr√∂√üer $R^2$, desto besser ist die Vorhersage. 
Da es ein Anteilsma√ü ist, 
liegt der Wertebereich zwischen 0 und 1.
Im Nullmodell betr√§gt R-Quadrat per Definition Null.
Im Fall von Modellen des Typs $y\sim x$ gilt: $R^2 = r_{xy}^2$.
$\square$
:::


Einfach gesagt: $R^2$ gibt an, wie gut (zu welchem Anteil) 
ein Modell die Zielvariable, $y$, erkl√§rt. 
Wir k√∂nnen R-Quadrat ($R^2$) uns von R z.B. so ausgeben lassen:

```{r}
r2(lm1)
```

Bei einer perfekten Korrelation ist $r=1$, 
daher ist dann auch $R^2 = 1$, 
vgl. @fig-r2-extreme.


```{r}
#| echo: false
#| fig-cap: "Extremf√§lle von R-Quadrat: 0 und 1. (a) Keine Korrelation, r = 0 und R2 = 0. Prognose durch Mittelwert; die Regressionsgerade ist (ungef√§hr) parallel zur X-Achse. (b) Perfekte Korrelation, r = 1 und $R2$ = 1: Die Prognose ist gleich dem beobachtetem Wert."
#| fig-subcap:
#|   - "Keine Korrelation"
#|   - "Perfekte Korrelation"
#| label: fig-r2-extreme
#| layout: [[45,-10, 45], [100]]


d0 <-
  tibble(x = rnorm(100),
         y = rnorm(100))

d0 %>% 
  ggplot(aes(x,y)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = modelcol) +
  theme_modern() + 
theme_large_text()


d_r1 =
  tibble(x = 1:10,
         y = seq(10, 100, by = 10))

d_r1 %>% 
  ggplot(aes(x,y)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = modelcol) +
  theme_modern() + 
theme_large_text()
```

Bei einer perfekten Korrelation $R^2=1$ liegen die Punkte auf der Geraden.
Im gegenteiligen Extremfall von $R^2=0$ ist die Vorhersage genauso gut, 
wie wenn man f√ºr jedes $y$ den Mittelwert, [$\color{ycol}{\bar{y}}$]{.content-visible unless-format="epub"}
[${\bar{y}}$]{.content-visible when-format="epub"}, vorhersagen w√ºrde. 
Je gr√∂√üer R-Quadrat, desto besser passt das Modell zu den Daten; 
desto besser "erkl√§rt" das Modell die Daten 
(desto besser der "Fit", sagt man).

::: {.content-visible when-format="html"}
[Diese App der FOM-Hochschule](https://fomshinyapps.shinyapps.io/Variationszerlegung/) erlaubt es Ihnen mit der Gr√∂√üe der Residuen eines linearen Modells zu spielen.
:::



<!-- ### Addition der Varianzen -->


<!-- Nennen wir die Varianz des Verkaufspreis $s^2_y$, die Verbesserung der Fehlerstreuung durch das *M*odell $s^2_m$ und die restliche Fehlerstreuung, den MSE, $s^2_e$. -->
<!-- Dann gilt: -->

<!-- $$s^2_y = s^2_m + s^2_e \\ -->
<!-- s^2_m = s^2_y - s^2_e$$ -->

<!-- ```{r} -->
<!-- s2_y = var(noten2$y) -->
<!-- s2_e = mse(lm1) -->
<!-- s2_m = s2_y - s2_e -->
<!-- s2_m -->
<!-- ``` -->

<!-- Die Varianzanteile addieren sich. Mit anderen Kennzahlen der Streuung (SD, MAE) funktioniert das nicht. -->


## Interpretation eines Regressionsmodells {#sec-interpret-reg-mod}


### Modellg√ºte

Die Residuen (Vorhersagefehler) bestimmen die Modellg√ºte:
Sind die Residuen im Schnitt gro√ü, so ist die Modellg√ºte gering (schlecht), und umgekehrt.
Verschiedenen Koeffizienten stehen zur Verf√ºgung: R-Quadrat, $r$ (als Korrelation von tats√§chlichem $y$ und vorhergesagten $\hat{y}$), MSE, RMSE, MAE, ...


### Koeffizienten

Die Modellkoeffizienten, also Achsenabschnitt ($\beta_0$) und Steigung ($\beta_1$) sind nur eingeschr√§nkt zu interpretieren, 
wenn man die zugrundeliegenden kausalen Abh√§ngigkeiten nicht kennt.
Allein aufgrund eines statistischen Zusammenhangs darf man keine kausalen Abh√§ngigkeiten annehmen.
Ohne eine zugrundeliegende Theorie f√ºr eine Kausalbehauptung kann man kann nur *deskriptiv* argumentieren.
Oder sich mit der Modellg√ºte und den Vorhersagen begn√ºgen. 
Was auch was wert ist.



[Im Modell `lm1` liegt der Achsenabschnitt bei $\textcolor{ycol}{y}=`r round(coef(lm1)[1], 2)`$. 
Beobachtungen mit $\color{xcol}{x}=0$ 
k√∂nnen also diesen $\textcolor{ycol}{Y}$-Wert erwarten, laut `lm1`.]{.content-visible unless-format="epub"}
[Im Modell `lm1` liegt der Achsenabschnitt (engl. intercept) bei ${y}=`r round(coef(lm1)[1], 2)`$.
Beobachtungen mit ${x}=0$ 
k√∂nnen also diesen ${Y}$-Wert erwarten, laut `lm1`.]{.content-visible when-format="epub"}
Leider ist es h√§ufig so, dass Pr√§diktorwerte von 0 in der Praxis nicht realistisch sind, 
so dass der Achsenabschnitt dann wenig n√ºtzt.

:::{#exm-groesse}
### Regression Gr√∂√üe und Gewicht
Nutzt man K√∂rpergr√∂√üe und das Gewicht von Menschen vorherzusagen, ist der Achsenabschnitt von K√∂rpergr√∂√üe wenig n√ºtzlich, da es keine Menschen gibt der Gr√∂√üe 0. $\square$
:::



```{r}
#| echo: false
lm1_b1 <- coef(lm1)[2] %>% round(2)
```

So interpretiert man die Geradensteigung, $\beta_1$:
"Im Modell `lm1` betr√§gt der Regressionskoeffizient $\beta_1 = `r lm1_b1`$. Zwei Studentinnen, deren Lernzeit sich um eine Stunde unterscheidet, unterscheiden sich *laut Modell* um den Wert von $\beta_1$".

:::{.callout-caution}
H√§ufig liest man, der "Effekt des Pr√§diktors" auf die AV betrage z.B. $`r lm1_b1`$. "Effekt" ist aber ein Wort, 
das man kausal verstehen kann.
Ohne weitere Absicherung kann man aber Regressionskoeffizienten nicht kausal verstehen. 
Daher sollte man das Wort "Effekt" mit Vorsicht genie√üen. 
Manche sprechen daher auch von einem "statistischen Effekt". $\square$
:::


## Wie man mit Statistik l√ºgt


Der Unterschied in Modellg√ºte zwischen, 
sagen wir, $r=.1$ und $r=.2$ ist *viel kleiner* als zwischen $r=.7$ und $r=.8$.
$R^2$ ist ein (lineares) Ma√ü der Modellg√ºte und da $r = \sqrt{R^2}$, 
d√ºrfen Unterschiede in $r$ nicht auf die gleiche Weise interpretiert werden wie Unterschiede in $R^2$. 
@fig-r-r2 zeigt den Zusammenhang von $r$ und $R^2$.


```{r}
#| echo: false
#| fig-cap: "Der Zusammenhang von r und R-Quadrat ist nicht linear."
#| label: fig-r-r2
#| out-width: 50%
#| fig-align: "center"

d <-
  tibble(r = seq(from = -1, to = 1, by = .01))

d <-
  d |> 
  mutate(r2 = r^2)


d |> 
  ggplot(aes(x = r, y = r2)) +
  geom_line() +
  theme_modern() +
  labs(y = "R-Quadrat") + 
  theme_large_text()
```


:::{.callout-caution}
Unterschiede zwischen Korrelationsdifferenzen d√ºrfen nicht linear interpretiert werden. $\square$
:::


## Fallbeispiel Mariokart

### Der Datenwahrsager legt los

Als mittlerweile anerkannter Extrem-Datenanalyst in dem Online-Auktionshaus, 
in dem Sie arbeiten, 
haben Sie sich neue Ziele gesetzt.
Sie m√∂chten eine genaue Vorhersage von Verkaufspreisen erzielen.
Als Sie von diesem Plan berichteten, leuchteten die Augen Ihrer Chefin.
Genaue Vorhersagen sind von hoher betriebswirtschaftlicher Relevanz.
Mariokart-Daten laden, am besten ohne Extremwerte, s. @lst-no-extreme 
und los geht's (und die √ºblichen Pakete starten, nicht vergessen)!

::: {.content-visible when-format="html"}

```{r}
mariokart <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv")
```
:::



```{r}
lm2 <- lm(total_pr ~ start_pr, data = mariokart)
r2(lm2)
```


Oh nein! Unterirdisch schlecht. Anstelle von blo√üem Rumprobieren √ºberlegen Sie und schauen dann nach, welche Variable am st√§rksten korreliert mit `total_pr`;
es resultiert `lm3`: 

```{r}
#| results: hide
lm3 <- lm(total_pr ~ ship_pr, data = mariokart)
parameters(lm3)
```

```{r}
#| echo: false
#| label: tbl-lm3
#| tbl-cap: "Modellparameter von lm3"
parameters(lm3) %>% print_md()
```


Der Achsenabschnitt liegt bei ca. `r round(coef(lm3)[1])` Dollar, 
wie man in @tbl-lm3 sieht: Ein Spiel, 
das mit null Dollar Preis startet, kann laut `lm3` etwa  `r round(coef(lm3)[1])` Dollar finaler Verkaufspreis erwarten.
*Pro Dollar an Versandkosten* (`ship_pr`) steigt der zu erwartende finale Verkaufspreis 
um ca. `r round(coef(lm3)[2])` Dollar.
(Die Spalte `95 CI` gibt einen Sch√§tzbereich f√ºr den jeweiligen Modellkoeffizienten an,
denn es handelt sich bei den Koeffizienten um Sch√§tzwerte;
der wahre Wert in der Population ist unbekannt. 
Wir kennen schlie√ülich nur eine Stichprobe der Gr√∂√üe $n = `r nrow(mariokart)`$.)
Die Regressionsgleichung von `lm3` lautet demnach:
`total_pr_pred = `r round(coef(lm3)[1])` + `r round(coef(lm3)[2])`*ship_pr`.

In Worten: 

>    Der vorhergesagte Gesamptreis eines Spiels liegt bei `r round(coef(lm3)[1])` Dollar "Sockelbetrag" plus `r round(coef(lm3)[2])` mal die Versandkosten.


### Vertiefung

Man kann sich die erwarteten Werte ("expectations") des Verkaufspreises in Abh√§ngigkeit vom Wert der UV (`ship_pr`) auch sch√§tzen ("to estimate") lassen, und zwar so mit `estimate_expectation(lm3)`, s. @tbl-lm3-expect.





```{r}
#| echo: false
#| label: tbl-lm3-expect
#| tbl-cap: "Die vorhergesagten (predicted) Werte und die Abweichungen vom vorhergesagten Wert (Residuals)"
estimate_expectation(lm3) %>% head() |> print_md()  # nur die ersten paar vorhergesagten Werte
```

```{r}
#| echo: false

pred_value <- predict(lm3, newdata=tibble(ship_pr = 4)) |> round()
```


"Ah, bei 4 Dollar Versandkosten ist laut dem Modell knapp `r pred_value` Dollar Verkaufspreis zu erwarten", 
fassen Sie sich die Ausgabe zusammen.


>    [ü§ñ]{.content-visible when-format="html"}[\emoji{robot}]{.content-visible when-format="pdf"} Das sieht man in der Spalte `Predicted`, dort steht der vorhersagte Wert f√ºr `total_pr` f√ºr einen bestimmten Wert von `ship_pr`.


>    [üßë‚Äçüéì]{.content-visible when-format="html"}[\emoji{student}]{.content-visible when-format="pdf"} Kann ich auch `predict` benutzen? Ich w√ºrde gerne den Verkaufspreis wissen, wenn die Versandkosten bei 1 und bei 4 Dollar liegen.

>    [ü§ñ]{.content-visible when-format="html"}[\emoji{robot}]{.content-visible when-format="pdf"} Ja, klar!


```{r}
neue_daten <- tibble(
  ship_pr = c(1, 4)) # zwei Werte zum Vorhersagen
```


```{r}
predict(lm3, newdata = neue_daten)
```




Aber n√ºtzlich w√§re noch, das Modell 
(bzw. die Sch√§tzung der erwarteten Werte) als Diagramm zu bekommen.
Das erreicht man z.B. so, s. @fig-lm3.

```{r}
#| label: fig-lm3
#| fig-cap: Verbildlichung der erwarteteten Werte laut lm3
#| out-width: 75%
estimate_expectation(lm3) %>% plot()
```

`estimate_expectation` hei√üt sinngem√§√ü "sch√§tze den zu erwartenden Wert".
Kurz gesagt: Wir wollen eine Vorhersage von R.

Am wichtigsten ist Ihnen aber im Moment die Frage, wie "gut" das Modell ist, spricht wie lang oder kurz die (absoluten) Vorhersagefehler-Balken sind:

```{r}
mae(lm3)
```


```{r}
#| echo: false
lm3_r2 <- round(r2(lm3)$R2, 2)
```

Das Modell erkl√§rt einen Anteil von ca. `r lm3_r2` der Gesamtstreuung.

```{r}
r2(lm3)
```



```{r}
mae(lm3)
```



Im n√§chsten Meeting erz√§hlen Sie Ihrem Chef 
"Ich kann den Verkaufspreis von Mariokart-Spielen im Schnitt auf `r round(mae(lm3),0)` 
Dollar genau vorhersagen!".
H√∂rt sich gut an.
Allerdings h√§tte ihr Chef es gerne genauer. 
Kann man da noch was machen?




## Fallstudie Immobilienpreise



{{< include children/casestudy-ames.qmd >}}




<!-- ## Fazit -->
<!-- TODO  -->

## Aufgaben



Die Webseite [datenwerk.netlify.app](https://datenwerk.netlify.app) stellt eine Reihe von einschl√§gigen √úbungsaufgaben bereit. 
Sie k√∂nnen die Suchfunktion der Webseite nutzen, 
um die Aufgaben mit den folgenden Namen zu suchen:


- Aussagen-einfache-Regr
- interpret-koeff-lm
- korr-als-regr
- Linearitaet1a
- lm1
- mtcars-regr01
- nichtlineare-regr1
- penguins-regr02
- regression1
- regression1b
- Regression3
- Regression4
- Regression5
- Regression6
- ames-kaggle1    
    

Schauen Sie sich auch weitere Aufgaben des [Datenwerks](https://sebastiansauer.github.io/Datenwerk/) an, 
vor allem mit den Tags [regression](https://sebastiansauer.github.io/Datenwerk/#category=regression) und [lm](https://sebastiansauer.github.io/Datenwerk/#category=lm).

*Nicht alle Aufgaben* aus dieser Sammlung passen zum Stoff dieses Kapitels; 
vielleicht k√∂nnen Sie einige Aufgaben nicht l√∂sen.
Ignorieren Sie einfach diese Aufgaben.



## Literaturhinweise

@gelman_regression_2021 liefert eine deutlich umfassendere Einf√ºhrung 
in die Regressionsanalyse als dieses Kapitel es tut.
Eine moderne, R-orientierte Einf√ºhrung in Statistik inklusive der Regressionsanalyse findet sich bei @cetinkaya-rundel_introduction_2021.
Ein Klassiker mit viel Aha-Potenzial ist @cohen_applied_2003.






